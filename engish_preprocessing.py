# -*- coding: utf-8 -*-
"""Engish_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11J5XD8kqKoMIJiPacYhbtRjU2w8vdVOA

# NLTK Package
"""

!pip install nltk

import nltk

dir(nltk)

"""# Importing Necessary Libraries"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""# Text Example"""

# Text Example
text = "Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. It is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning."

"""## Lower Section"""

## Lowersection
text = text.lower()

"""## Punctuation"""

# Punctuantions
punctuations = [string.punctuation]
punctuations

text = text.translate(str.maketrans('', '', string.punctuation))
text

"""## Tokenization"""

# Tokenization
tokens = word_tokenize(text)
tokens

"""# Remove Stop Words"""

# Stop words removal
stop_words = set(stopwords.words('english'))
stop_words

filtered_tokens = [word for word in tokens if word not in stop_words]
filtered_tokens

"""## Stemming"""

# Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
stemmed_tokens

"""## Lemmatization"""

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]
lemmatized_tokens